{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\PUJA KUMARI\\\\Desktop\\\\SourceCodeAnalysis\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir text_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clone the repo inside text_repo\n",
    "repo_path = \"test_repo/\"\n",
    "\n",
    "repo = Repo.clone_from(\"https://github.com/Pujakumari1202/audio_transcript.git\", to_path=repo_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the files (extract all the python file)  \n",
    "# give the loader object\n",
    "loader=GenericLoader.from_filesystem(repo_path,\n",
    "                                      glob=\"**/*\",\n",
    "                                      suffixes=[\".py\"],\n",
    "                                      parser=LanguageParser(language=Language.PYTHON,parser_threshold=500)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now load the  documents\n",
    "documents=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='import openai\\nimport os\\nfrom dotenv import load_dotenv\\nfrom flask import Flask, request, jsonify, render_template\\n\\n\\n# load the api key\\nload_dotenv()\\nOPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\")\\n\\n#add this openai key inside my openai key\\nopenai.api_key=OPENAI_API_KEY\\n\\n\\n#initialize the flask\\napp = Flask(__name__)\\n\\n\\n# audio will be upload in static folder\\napp.config[\"UPLOAD_FOLDER\"]=\"static\"\\n\\n\\n#create the route look for 2 kinds of req get and post\\n@app.route(\"/\", methods=[\"GET\",\"POST\"])\\n\\n#if this match then it will execute this main function\\ndef main():\\n    # if the req is post then\\n    if request.method ==\"POST\":\\n        #take the language input \\n        language= request.form[\"language\"]\\n\\n        #get the file\\n        file= request.files[\"file\"]\\n\\n        # if file is present or not check\\n        if file:  # save in static folder\\n            filename=file.filename   #give the file name\\n            file.save(os.path.join(app.config[\\'UPLOAD_FOLDER\\'],filename))\\n\\n           # load the audio\\n            audio_file =open(\"static/recorded.mp3\",\"rb\")\\n            transcript=openai.Audio.translate(\"whisper-1\",audio_file)\\n\\n\\n            # now using the gpt-4 model convert to the transilation \\n            response = openai.ChatCompletion.create(\\n                    model=\"gpt-4\",\\n                    messages = [{ \"role\": \"system\", \"content\": f\"You will be provided with a sentence in English, and your task is to translate it into {language}\" }, \\n                                { \"role\": \"user\", \"content\": transcript.text }],\\n                    temperature=0,\\n                    max_tokens=256\\n                  )\\n            \\n            return jsonify(response)\\n    \\n\\n    # if it is get req then noly render the web app\\n    return render_template(\"index.html\")\\n\\n\\n\\n#run on that post\\nif __name__ == \"__main__\":\\n    app.run(host=\"0.0.0.0\", debug=True, port=8080)\\n        \\n    \\n\\n', metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# how to use whisper model\\nimport openai\\nimport os\\nfrom dotenv import load_dotenv\\n\\n\\n# load the api key\\nload_dotenv()\\nOPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\")\\n\\n#add this openai key inside my openai key\\nopenai.api_key=OPENAI_API_KEY\\n\\n\\n#open that audio file (rb=read binary)\\naudio_file= open(\"recorded.mp3\",\"rb\")\\n\\n\\n# get the transcript used that openai whisper model(pass the model and audio)\\noutput=openai.Audio.translate(\"whisper-1\",audio_file)\\n\\n\\nprint(output)\\n', metadata={'source': 'test_repo\\\\demo.py', 'language': <Language.PYTHON: 'python'>})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now see the length of the documents\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='import openai\\nimport os\\nfrom dotenv import load_dotenv\\nfrom flask import Flask, request, jsonify, render_template\\n\\n\\n# load the api key\\nload_dotenv()\\nOPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\")\\n\\n#add this openai key inside my openai key\\nopenai.api_key=OPENAI_API_KEY\\n\\n\\n#initialize the flask\\napp = Flask(__name__)\\n\\n\\n# audio will be upload in static folder\\napp.config[\"UPLOAD_FOLDER\"]=\"static\"\\n\\n\\n#create the route look for 2 kinds of req get and post\\n@app.route(\"/\", methods=[\"GET\",\"POST\"])\\n\\n#if this match then it will execute this main function\\ndef main():\\n    # if the req is post then\\n    if request.method ==\"POST\":\\n        #take the language input \\n        language= request.form[\"language\"]\\n\\n        #get the file\\n        file= request.files[\"file\"]\\n\\n        # if file is present or not check\\n        if file:  # save in static folder\\n            filename=file.filename   #give the file name\\n            file.save(os.path.join(app.config[\\'UPLOAD_FOLDER\\'],filename))\\n\\n           # load the audio\\n            audio_file =open(\"static/recorded.mp3\",\"rb\")\\n            transcript=openai.Audio.translate(\"whisper-1\",audio_file)\\n\\n\\n            # now using the gpt-4 model convert to the transilation \\n            response = openai.ChatCompletion.create(\\n                    model=\"gpt-4\",\\n                    messages = [{ \"role\": \"system\", \"content\": f\"You will be provided with a sentence in English, and your task is to translate it into {language}\" }, \\n                                { \"role\": \"user\", \"content\": transcript.text }],\\n                    temperature=0,\\n                    max_tokens=256\\n                  )\\n            \\n            return jsonify(response)\\n    \\n\\n    # if it is get req then noly render the web app\\n    return render_template(\"index.html\")\\n\\n\\n\\n#run on that post\\nif __name__ == \"__main__\":\\n    app.run(host=\"0.0.0.0\", debug=True, port=8080)\\n        \\n    \\n\\n', metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the chunks  \n",
    "documents_splitter = RecursiveCharacterTextSplitter.from_language(language = Language.PYTHON,\n",
    "                                                             chunk_size = 500,\n",
    "                                                             chunk_overlap = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply on the entire documents \n",
    "texts=documents_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='import openai\\nimport os\\nfrom dotenv import load_dotenv\\nfrom flask import Flask, request, jsonify, render_template\\n\\n\\n# load the api key\\nload_dotenv()\\nOPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\")\\n\\n#add this openai key inside my openai key\\nopenai.api_key=OPENAI_API_KEY\\n\\n\\n#initialize the flask\\napp = Flask(__name__)\\n\\n\\n# audio will be upload in static folder\\napp.config[\"UPLOAD_FOLDER\"]=\"static\"\\n\\n\\n#create the route look for 2 kinds of req get and post\\n@app.route(\"/\", methods=[\"GET\",\"POST\"])', metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='#if this match then it will execute this main function', metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='def main():\\n    # if the req is post then\\n    if request.method ==\"POST\":\\n        #take the language input \\n        language= request.form[\"language\"]\\n\\n        #get the file\\n        file= request.files[\"file\"]\\n\\n        # if file is present or not check\\n        if file:  # save in static folder\\n            filename=file.filename   #give the file name\\n            file.save(os.path.join(app.config[\\'UPLOAD_FOLDER\\'],filename))', metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# load the audio\\n            audio_file =open(\"static/recorded.mp3\",\"rb\")\\n            transcript=openai.Audio.translate(\"whisper-1\",audio_file)', metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# now using the gpt-4 model convert to the transilation \\n            response = openai.ChatCompletion.create(\\n                    model=\"gpt-4\",\\n                    messages = [{ \"role\": \"system\", \"content\": f\"You will be provided with a sentence in English, and your task is to translate it into {language}\" }, \\n                                { \"role\": \"user\", \"content\": transcript.text }],\\n                    temperature=0,\\n                    max_tokens=256\\n                  )', metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content=')\\n            \\n            return jsonify(response)', metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# if it is get req then noly render the web app\\n    return render_template(\"index.html\")\\n\\n\\n\\n#run on that post\\nif __name__ == \"__main__\":\\n    app.run(host=\"0.0.0.0\", debug=True, port=8080)', metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='# how to use whisper model\\nimport openai\\nimport os\\nfrom dotenv import load_dotenv\\n\\n\\n# load the api key\\nload_dotenv()\\nOPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\")\\n\\n#add this openai key inside my openai key\\nopenai.api_key=OPENAI_API_KEY\\n\\n\\n#open that audio file (rb=read binary)\\naudio_file= open(\"recorded.mp3\",\"rb\")\\n\\n\\n# get the transcript used that openai whisper model(pass the model and audio)\\noutput=openai.Audio.translate(\"whisper-1\",audio_file)\\n\\n\\nprint(output)', metadata={'source': 'test_repo\\\\demo.py', 'language': <Language.PYTHON: 'python'>})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#entire chunks\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need embedding model for that set openai_api key\n",
    "# load the api key\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY=os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the api key as our environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialized the embedding model\n",
    "embeddings=OpenAIEmbeddings(disallowed_special=())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now initialised our chromedb\n",
    "vectordb=Chroma.from_documents(texts,embedding=embeddings,persist_directory='./db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activate the vectordb\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialized our llm model we can use any llm model\n",
    "# llm=ChatOpenAI(model_name=\"gpt-4\")\n",
    "llm=ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the memory\n",
    "memory=ConversationSummaryMemory(llm=llm,memory_key=\"chat_history\",return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating our final chain\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":8}), memory=memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "question =\"What is  jsonify function in flask?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 16, updating n_results = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `jsonify` function in Flask serializes data into JSON format and wraps it in a Response object. This function is commonly used to return JSON responses from Flask routes.\n"
     ]
    }
   ],
   "source": [
    "# pass this question inside qa\n",
    "result=qa(question)\n",
    "\n",
    "#Extract the answer from the result\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "question =\"What is  load_dotenv function?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 16, updating n_results = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `load_dotenv` function is used to load environmental variables from a .env file into the environment. It reads the key-value pairs from the .env file and sets them as environment variables, making them accessible to the application.\n"
     ]
    }
   ],
   "source": [
    "# pass this question inside qa\n",
    "result=qa(question)\n",
    "\n",
    "#Extract the answer from the result\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
